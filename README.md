# keras-fractalnet
FractalNet implementation in Keras

In CIFAR-10, it shows similar results as in the paper. Going to update this readme with actual numbers when I have more time.

## Model

Model graph image of FractalNet(c=3, b=5) generated by keras: [link](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/model.png)

## Results

This results are from the experiments with the code published here. The authors of the paper have not yet released the code as of the publishing of this so I can't say what's different from theirs code.

Test error (%)

Method | C10
------ | ---
ResNet (reported by [1]) | 13.63
ResNet Stocatic Depth (reported by [1]) | 11.66
FractalNet (paper)                         | 10.18
FractalNet+dropout/drop-path (paper)       | 7.33
FractalNet+dropout/drop-path (this w/Adam) | 8.73
FractalNet+dropout/drop-path (this w/SGD)  | 9.80

[1] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger.  Deep networks with stochastic depth. arXiv preprint arXiv:1603.09382, 2016.

### CIFAR-10

Training as reported by the paper with SGD for 400 epochs starting with 0.02 learning rate and reducing it by 10x each time it reaches half of the remaining epochs (200, 300, 350, 375).

![](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/c10_loss_train_sgd.png)


Training with Adam with default parameters for 400 epochs.

![](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/c10_loss_train_adam.png)

## Paper

arXiv: [FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648)

    @article{larsson2016fractalnet,
      title={FractalNet: Ultra-Deep Neural Networks without Residuals},
      author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
      journal={arXiv preprint arXiv:1605.07648},
      year={2016}
    }
