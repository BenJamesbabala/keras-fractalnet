# keras-fractalnet
FractalNet implementation in Keras

In CIFAR-10, it shows similar results as in the paper. Going to update this readme with actual numbers when I have more time.

## Model

Model graph image of FractalNet(c=3, b=5) generated by keras: [link](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/model.png)

## Results

This results are from the experiments with the code published here. The authors of the paper have not yet released the code as of the publishing of this so I can't say what's different from theirs code. Also there is no kind of normalization across the dataset in the raw tests (which they may have done).

So far the results are promising when compared against Residual Networks. But I couldn't reproduce their deepest-column experiment.

The code here might have bugs too, if you find anything write me or submit a PR and I will rerun the tests.

Test error (%)

Method | C10 | C100
------ | --- | ----
ResNet (reported by [1]) | 13.63 | 44.76
ResNet Stocatic Depth (reported by [1]) | 11.66 | 37.80
FractalNet (paper)                         | 10.18 | 35.34
FractalNet+dropout/drop-path (paper w/SGD)       | 7.33 | 28.20
FractalNet+dropout/drop-path (this w/SGD)  | 11.16 | 34.99
FractalNet+dropout/drop-path (this w/Adam) | 8.73 | N/A
FractalNet+dropout/drop-path/deepest-column (paper w/SGD) | 7.27 | 29.05
FractalNet+dropout/drop-path/deepest-column (this w/SGD) | 21.07 | N/A
FractalNet+dropout/drop-path/deepest-column (this w/Adam) | N/A | N/A

[1] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger.  Deep networks with stochastic depth. arXiv preprint arXiv:1603.09382, 2016.

### CIFAR-10

Training as reported by the paper with SGD for 400 epochs starting with 0.02 learning rate and reducing it by 10x each time it reaches half of the remaining epochs (200, 300, 350, 375).

![](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/c10_loss_train_sgd.png)


Training with Adam with default parameters for 400 epochs.

![](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/c10_loss_train_adam.png)

### CIFAR-100

Trained with SGD (as with CIFAR-10):

![](https://raw.githubusercontent.com/snf/keras-fractalnet/master/doc/c100_loss_train_sgd.png)

## Paper

arXiv: [FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648)

    @article{larsson2016fractalnet,
      title={FractalNet: Ultra-Deep Neural Networks without Residuals},
      author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
      journal={arXiv preprint arXiv:1605.07648},
      year={2016}
    }
